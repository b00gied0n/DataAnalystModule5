{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "#### What is Linear Regression?\n",
    "- Linear Regression is a powerful statistical analytical method that allows us to examine the relationship between two or more variables of interest, the relationship between independent and dependent variables.\n",
    "- Linear Regression examines the effects of one or more independent variables on a dependent variable.\n",
    "- Linear regression is a supervised learning algorithm.\n",
    "- Applications of Linear Regression range from predicting health outcomes in medicine, stock prices in finance, power usage in high-performance computing, marketing effectiveness on pricing and promotions and sales of a products.\n",
    "\n",
    "\n",
    "#### Format of Linear Regression formula\n",
    "\n",
    "- In Multiple Linear Regression, a Multiple independent variables(x1,x2,x3,...xn) are used to predict the value of a single dependent variable(y)\n",
    "\n",
    "![Regression Formula](pics/MLRFormula.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Goal\n",
    "The goal of this lab is to predict the sales price for all new houses getting build."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the \"HousePrices\" dataset\n",
    "The House Prices dataset contains 100 observations and 5 different attributes (4 independent variables and 1 dependent variable)\n",
    "\n",
    "#### Independent Variables\n",
    "    1. House Sqft – square footage of the property (X1)\n",
    "    2. Taxes - property tax will be calculated on this value (X2)\n",
    "    3. Bedrooms – number of bedrooms in the property (X3)\n",
    "    4. Bathrooms – number of bathrooms in the property (X4)\n",
    "\n",
    "#### Dependent Variable\n",
    "    5. Last Sold Price - the value the property got sold for(Y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Install Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install scikit-learn\n",
    "#!pip install scipy\n",
    "#!pip install seaborn\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing some common libraries that’s needed for all data science related projects\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "\n",
    "\n",
    "# Importing different modules from the sklearn library to build and evaluate the linear regression model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Importing matplotlib and seaborn libraries for data visualisation \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Switching off unnecessary warning messages \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process map\n",
    "Below illustrates a 14-step process used during this lab.\n",
    "\n",
    "    1.\tImport Data\n",
    "    2.\tData Quality Checks\n",
    "    3.\tData Cleansing\n",
    "    4.\tExploratory Analysis using Aggregations\n",
    "    5.\tExploratory Analysis using Distributions\n",
    "    6.\tExploratory Analysis using Correlations\n",
    "    7.\tVisualisations\n",
    "    8.\tModel: Pre-processing\n",
    "    9.\tModel: Train/Test Split\n",
    "    10.\tModel: Build (Train dataset)\n",
    "    11.\tModel: Evaluation (Train dataset)\n",
    "    12.\tModel: Evaluation (Test dataset)\n",
    "    13.\tModel: Predictions\n",
    "    14.\tModel: Save Predictions\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from a CSV file and saving that data into a dataframe called \"df\"\n",
    "\n",
    "df = pd.read_csv(\"HousePrices.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Quality Checks\n",
    "\n",
    "    2.1 Check data\n",
    "    2.2 Check shape of data\n",
    "    2.3 Check for duplicates\n",
    "    2.4 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "# Viewing top 5 records\n",
    "df.head()\n",
    "\n",
    "# Viewing last 5 records\n",
    "#df.tail()\n",
    "\n",
    "# Viewing top 3 records\n",
    "#df.head(n=3)\n",
    "\n",
    "# Viewing last 3 records\n",
    "#df.tail(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2\n",
    "# Looking at the structure of the dataframe\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3\n",
    "# Let’s use duplicated() function to identify how many duplicate records there are in the dataset\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2.4\n",
    "# This method prints out information about a dataframe including the index, dtype, columns, non-null values and memory usage\n",
    "# This method is also useful for finding out missing values in a dataset\n",
    "# if found, we can use interpolation techniques to rectify those missing values\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Cleansing\n",
    "\n",
    "    3.1 Converting data types\n",
    "    3.2 Remove duplicates\n",
    "    3.3 Fill missing values\n",
    "    3.4 Outlier detection and treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1\n",
    "# Converting data type of a column using astype() method\n",
    "\n",
    "df[\"HouseSqft\"] = df.HouseSqft.astype(\"float64\")\n",
    "df[\"Taxes\"] = df.Taxes.astype(\"float64\")\n",
    "df[\"Bedrooms\"] = df.Bedrooms.astype(\"category\")\n",
    "df[\"Bathrooms\"] = df.Bathrooms.astype(\"category\")\n",
    "df[\"LastSoldPrice\"] = df.LastSoldPrice.astype(\"int64\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2\n",
    "# This is how you remove all the duplicates from the dataset using drop_duplicates() function\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3.3\n",
    "# Fill missing values (NaN, Null) with median value of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you calculate median for all columns in the dataframe\n",
    "df.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you calculate median value for a specific column\n",
    "df.HouseSqft.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you fix missing values for all columns\n",
    "# df = df.fillna(df.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is how you fix a missing value for a specific column\n",
    "df.HouseSqft = df.HouseSqft.fillna(df.HouseSqft.median())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By looking at the info it is clear all the missing values are correctly replaced with median value\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4\n",
    "#### Outlier Detection and Treatment\n",
    "- One of the most important step in data cleansing is outlier detection and treatment.\n",
    "- Outliers are defined as data points that are significantly different from the remaining data. Those are points that lie outside the overall pattern of the distribution. Statistical measures such as mean, variance and correlation are very susceptible to outliers.\n",
    "\n",
    "#### Outlier Detection\n",
    "- This can be done through visualising the data (Box and whisker plot)\n",
    "\n",
    "\n",
    "#### Outlier Treatment\n",
    "- This can be done by imputing mean/median or random value in place of an outlier\n",
    "\n",
    "![boxplot](pics/boxplot1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Outlier detection using boxplot from seaborn library\n",
    "\n",
    "sns.boxplot(data=df[[\"HouseSqft\",\"Taxes\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier treatment\n",
    "# Calculation of Q1, Q3, IQR for \"HouseSqft\" column:\n",
    "\n",
    "q1 = np.percentile(df.HouseSqft,[25])[0]\n",
    "q3 = np.percentile(df.HouseSqft,[75])[0]\n",
    "iqr = q3-q1\n",
    "\n",
    "ll = q1 - iqr*1.5 #lower limit\n",
    "ul = q3 + iqr*1.5 #upper limit\n",
    "\n",
    "print(ll)\n",
    "print(ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option1 - Detecting outliers and imputing with custom values\n",
    "\n",
    "df.HouseSqft[df.HouseSqft>ul] = ul\n",
    "df.HouseSqft[df.HouseSqft<ll] = ll\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note the dataset contains 100 records after imputing outliers\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option2 - Filtering out outliers from the dataset\n",
    "# Please note the dataset only contains 95 records, 5 records are treated as outliers\n",
    "\n",
    "# df = df.loc[(df.HouseSqft<=ul) & (df.HouseSqft>=ll) , [\"HouseSqft\",\"Taxes\", \"Bedrooms\",\"Bathrooms\",\"LastSoldPrice\"]]\n",
    "\n",
    "# This code below can also create the same output as above\n",
    "# By omitting column section, we can display all columns\n",
    "# df.loc[(df.HouseSqft<=ul) | (df.HouseSqft>=ll)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Exploratory Analysis using Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of houses and mean house price\n",
    "df.agg({\"LastSoldPrice\": ['count', 'mean']})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Exploratory Analysis using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean house price by bedrooms and bathrooms\n",
    "\n",
    "df.groupby(by=[\"Bedrooms\", \"Bathrooms\"]).agg({\"LastSoldPrice\": ['count','mean']}).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Descriptive statistics include those that summarise the central tendency, \n",
    "# dispersion and shape of a dataset’s distribution, excluding NaN(Not a Number) values\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Exploratory Analysis using Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the valuable aspects of regression, is that it’s able to deal with some amount of correlation among independent variables. However, too much multicollinearity in the data can be a problem\n",
    "- Multicollinearity arises when two variables that measure the same thing or similar things (e.g., weight and BMI) are both included in a multiple regression model. They will, in effect, cancel each other out and generally destroy your model\n",
    "- The main goal: choose independent variables that are highly correlated with the dependent variable (they provide information), but that are not highly correlated with other independent variables (the same information is not repeated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating correlation Matrix\n",
    "\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code can be used to remove any columns due to multicollinearity\n",
    "# df = df.drop([\"Taxes\"], axis=1)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joinplot from seaborn library can be used to create a scatterplot\n",
    "\n",
    "sns.jointplot(x=df.HouseSqft, y=df.LastSoldPrice, kind=\"reg\")\n",
    "plt.xlabel('HouseSqft')\n",
    "plt.ylabel('LastSoldPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Model: Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding technique converts categorical data into numerical data\n",
    "\n",
    "![Encode](pics/Encode.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting categorical variables into dummy variables (one-hot encoding)\n",
    "\n",
    "df = pd.get_dummies(data=df, columns=[\"Bedrooms\", \"Bathrooms\"], drop_first=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for column names and other info\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Model: Train/Test Split \n",
    "\n",
    "**Step1: Split dataset to X and Y variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of independent variables and dependent variable\n",
    "\n",
    "x = df.loc[:, df.columns != \"LastSoldPrice\"]\n",
    "y = df.loc[:, df.columns == \"LastSoldPrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exploring all independent variables\n",
    "\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dependent variable\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the shape of x and y datasets - (no of rows, no of columns)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2: Performing 70:30 Data split**\n",
    "- After Separating columns into dependent and independent variables (x, y), you split those into training-set and testing-set (70:30)\n",
    "\n",
    "\n",
    "![split data](pics/traintestsplitdata1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into train and test datasets --> 70:30 split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dimensions of train datasets\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dimensions of test datasets\n",
    "\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Model: Build (Train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn library to build a Linear Regression Model\n",
    "# from sklearn.linear_model import LinearRegression --> this code imports the Linear Regression module\n",
    "\n",
    "\n",
    "# Create a linear regression model using LinearRegression() module\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the training data (70%) to the linear regression model\n",
    "# this will generate the intercept and all the coefficients\n",
    "\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Model: Evaluation (Train dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the intercept\n",
    "\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the coefficients\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see the above coefficients and intercept are very poorly formatted\n",
    "# the below is much better representation of \"intercept\" in a dataframe layout\n",
    "\n",
    "pd.DataFrame(np.array(model.intercept_), index=[\"Intercept\"], columns=[\"Intercept\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As you can see the above coefficients and intercept are very poorly formatted\n",
    "# the below is much better representation of \"coefficients\" in a dataframe layout\n",
    "\n",
    "pd.DataFrame(np.array(model.coef_).T, index=x.columns, columns=[\"Coefficients\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Model: Evaluation (Test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appling the linear regression model to make prediction on testing dataset(30%)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluating the above predicted results (model performance)\n",
    "\n",
    "print(\"Root Mean squared error (RMSE):{}\".format(math.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "print(\"Coefficient of determination (R^2):{}\".format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Model: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on new data\n",
    "\n",
    "# Reading data from a CSV file and saving that data as a dataframe\n",
    "dfp = pd.read_csv(\"HousePricesPredict.csv\")\n",
    "\n",
    "# Viewing records\n",
    "dfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method prints out information about a dataFrame including the index, dtype, columns, non-null values and memory usage\n",
    "# This method is also useful for finding out missing values in a dataset\n",
    "# if found, we can use interpolation techniques to rectify those missing values\n",
    "\n",
    "dfp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting data type of a column using astype() method\n",
    "\n",
    "dfp[\"HouseSqft\"] = dfp.HouseSqft.astype(\"float64\")\n",
    "dfp[\"Taxes\"] = dfp.Taxes.astype(\"float64\")\n",
    "dfp[\"Bedrooms\"] = dfp.Bedrooms.astype(\"category\")\n",
    "dfp[\"Bathrooms\"] = dfp.Bathrooms.astype(\"category\")\n",
    "dfp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s remove all the duplicates from the dataset\n",
    "\n",
    "dfp = dfp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s remove all the null values from the dataset\n",
    "\n",
    "dfp = dfp.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let’s convert categorical variables into dummy variables (one-hot encoding)\n",
    "\n",
    "dfp2 = pd.get_dummies(data=dfp,columns=[\"Bedrooms\", \"Bathrooms\"], drop_first=True)\n",
    "dfp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the structure of the Dataframe\n",
    "\n",
    "dfp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method prints out information about a dataFrame including the index, dtype, columns, non-null values and memory usage\n",
    "\n",
    "dfp2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new predictions using the \"model\" that was created in the earlier section\n",
    "\n",
    "newhouseprices = model.predict(dfp2)\n",
    "newhouseprices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting predicted results into a dataframe (\"dfr\")\n",
    "\n",
    "dfr = pd.DataFrame(newhouseprices, columns=[\"newhouseprices\"])\n",
    "dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attaching predicted prices to the original dataset, and save as a new dataframe (\"newdf\")\n",
    "\n",
    "newdf = pd.DataFrame.join(dfp,dfr)\n",
    "newdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Model: Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the above dataframe (\"newdf\") as a CSV file\n",
    "\n",
    "newdf.to_csv(\"NewHousePricesPredicted.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
